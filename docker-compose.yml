# LockClaw Baseline — Docker Compose
#
# Quick start (pulls pre-built image — no local build):
#   docker compose up -d openclaw
#
# Build from source instead:
#   docker compose up -d --build openclaw
#
# Default locked-down mode:
#   docker compose up -d openclaw

x-common: &common
  restart: unless-stopped
  read_only: true
  security_opt:
    - no-new-privileges:true
  cap_drop:
    - ALL
  tmpfs:
    - /tmp
    - /run
    - /var/tmp
  environment:
    - LOCKCLAW_MODE=${LOCKCLAW_MODE:-hobby}

services:
  # ── OpenClaw gateway + claude-mem ──────────────────────────
  openclaw:
    <<: *common
    image: ghcr.io/iwes247/lockclaw-baseline:openclaw
    build:
      context: .
      target: openclaw
    container_name: lockclaw-openclaw
    environment:
      - LOCKCLAW_MODE=${LOCKCLAW_MODE:-hobby}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - LOCKCLAW_DATA_DIR=/data
    volumes:
      - openclaw-data:/data
    # OpenClaw gateway is loopback-only inside the container.
    # Access via SSH tunnel:
    #   ssh -p 2222 -L 18789:127.0.0.1:18789 lockclaw@localhost

  # ── Ollama local LLM engine ───────────────────────────────
  ollama:
    <<: *common
    image: ghcr.io/iwes247/lockclaw-baseline:ollama
    build:
      context: .
      target: ollama
    container_name: lockclaw-ollama
    environment:
      - LOCKCLAW_MODE=${LOCKCLAW_MODE:-hobby}
      - LOCKCLAW_DATA_DIR=/data
      - OLLAMA_HOST=127.0.0.1:11434
      - OLLAMA_MODELS=/data/ollama/models
    volumes:
      - ollama-models:/data
    # Ollama API is loopback-only inside the container.
    # Access via SSH tunnel:
    #   ssh -p 2223 -L 11434:127.0.0.1:11434 lockclaw@localhost
    # For GPU passthrough, uncomment:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  openclaw-data:
    driver: local
  ollama-models:
    driver: local
