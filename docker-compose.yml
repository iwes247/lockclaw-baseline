version: "3.8"

# LockClaw Baseline â€” Docker Compose
# Start one or more AI runtime profiles.
#
# Usage:
#   docker compose up -d openclaw      # OpenClaw gateway
#   docker compose up -d ollama        # Ollama LLM engine
#   docker compose up -d               # both
#
# With SSH access:
#   LOCKCLAW_ENABLE_SSH=1 SSH_PUBLIC_KEY="$(cat ~/.ssh/id_ed25519.pub)" \
#     docker compose up -d openclaw

x-common: &common
  build:
    context: .
  restart: unless-stopped
  environment:
    - LOCKCLAW_ENABLE_SSH=${LOCKCLAW_ENABLE_SSH:-0}
    - SSH_PUBLIC_KEY=${SSH_PUBLIC_KEY:-}

services:
  openclaw:
    <<: *common
    build:
      context: .
      target: openclaw
    container_name: lockclaw-openclaw
    environment:
      - LOCKCLAW_ENABLE_SSH=${LOCKCLAW_ENABLE_SSH:-0}
      - SSH_PUBLIC_KEY=${SSH_PUBLIC_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    volumes:
      - openclaw-data:/home/lockclaw/.openclaw
    ports:
      # SSH only exposed if LOCKCLAW_ENABLE_SSH=1
      - "${LOCKCLAW_SSH_PORT:-2222}:22"
    # OpenClaw gateway is loopback-only inside the container.
    # Access via SSH tunnel:
    #   ssh -p 2222 -L 18789:127.0.0.1:18789 lockclaw@localhost

  ollama:
    <<: *common
    build:
      context: .
      target: ollama
    container_name: lockclaw-ollama
    environment:
      - LOCKCLAW_ENABLE_SSH=${LOCKCLAW_ENABLE_SSH:-0}
      - SSH_PUBLIC_KEY=${SSH_PUBLIC_KEY:-}
      - OLLAMA_HOST=127.0.0.1:11434
      - OLLAMA_MODELS=/home/lockclaw/.ollama/models
    volumes:
      - ollama-models:/home/lockclaw/.ollama
    ports:
      - "${LOCKCLAW_SSH_PORT_OLLAMA:-2223}:22"
    # Ollama API is loopback-only inside the container.
    # Access via SSH tunnel:
    #   ssh -p 2223 -L 11434:127.0.0.1:11434 lockclaw@localhost
    # For GPU passthrough, uncomment:
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

volumes:
  openclaw-data:
    driver: local
  ollama-models:
    driver: local
